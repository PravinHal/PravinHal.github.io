---
layout: page
title: About
permalink: /about/
---

<!--- ![Trenton B. Bricken](../images/TrentonBricken.jpg){:style="width: 200px; float: right; padding-left: 20px"}-->

Hi, I'm Trenton. I am very interested in convergences between machine learning and neuroscience. I am using associative memory models, particularly Sparse Distributed Memory, to investigate these convergences.

Information about me:
* I'm a PhD student in the Systems, Synthetic and Quantitative Biology (SSQB) Program at Harvard in the [Kreiman Lab](http://klab.tch.harvard.edu/publications/publications.html#sthash.dUJc7Kpv.dpbs) (started Sept. 2020).
* I graduated from Duke University in May 2020 with a self-made major in "Minds and Machines: Biological and Artificial Intelligence". I was lucky to attend as a [Robertson Scholar](https://robertsonscholars.org/) which provided full funding during all four years, including summer experiences.
* During my time at Duke I spent a year (June 2018 - May 2019) doing research in [Dr. Michael Lynch's Lab](https://lynchlab.pratt.duke.edu) at Duke attempting to use machine learning to design new CRISPR guide RNAs for safer, more effective genome editing. Afterwards, I was affiliated with [Dr. Debora Marks's Lab](https://marks.hms.harvard.edu) at Harvard Medical School, first as a summer intern and then through my Senior Thesis research.

I am involved in the movement/philosophy/set of ideas that is [Effective Altruism](https://www.effectivealtruism.org).

If the world was void of both interesting research questions and global catastrophic risks(!), you'd find me backpacking around the world with my [film camera](https://blitz-analog.github.io/). I still try to do this when I have time off and get the chance to travel somewhere cool.

## Publications (in reverse chronological order):

**Attention Approximates Sparse Distributed Memory**<br>
***Trenton Bricken***\*, Cengiz Pehlevan
\*(First author)<br>
NeurIPS, December 2021<br>
[[paper](https://arxiv.org/abs/2111.05498)] [[blog-post](https://www.trentonbricken.com/Attention-Approximates-Sparse-Distributed-Memory/)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1458465726503784449?s=20)]<br>

MIT Center for Brains Minds+ Machines Talk:<br>

<div align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/THIIk7LR9_8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

**High-content screening of coronavirus genes for innate immune suppression reveals enhanced potency of SARS-CoV-2 proteins.**<br>
Erika J Olson\*, David M Brown\*, Timothy Z Chang, Lin Ding, Tai L Ng, H. Sloane Weiss, Peter Koch, Yukiye Koide, Nathan Rollins, Pia Mach, Tobias Meisinger, ***Trenton Bricken***, Joshus Rollins, Yun Zhang, Colin Molloy, Yun Zhang, Briodget N Queenan, Timothy Mitchison, Debora Marks, Jeffrey C Way, John I Glass, Pamela A Silver<br>
\*(First authors)<br>
*bioRxiv, March 2021*<br>
[[preprint](https://www.biorxiv.org/content/10.1101/2021.03.02.433434v1)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1367141915666317312?s=20)]

**Computationally Optimized SARS-CoV-2 MHC Class I and II Vaccine Formulations Predicted to Target Human Haplotype Distributions.**<br>
Ge Liu\*, Brandon Carter\*, ***Trenton Bricken***, Siddhartha Jain, Mathias Viard, Mary Carrington, David K Gifford<br>
\*(First authors)<br>
*Cell Systems, July 2020*<br>
[[paper](https://www.cell.com/cell-systems/fulltext/S2405-4712%2820%2930238-6#%20)] [[code](https://github.com/gifford-lab/optivax)] [[preprint](https://www.biorxiv.org/content/10.1101/2020.05.16.088989v1)] [[tweet-thread](https://twitter.com/TrentonBricken/status/1262407888842170370?s=20)]

My Google Scholar profile can be found [here](https://scholar.google.com/citations?user=CP6aLusAAAAJ&hl=en).

## Resume

[My CV (last updated on Nov. 10 2021)](../documents/Trenton-Bricken-CV.pdf)

## Current Projects

* My current research is on the extent to which Deep Learning and the brain are convergent. I am using Sparse Distributed Memory, a biologically plausible associative memory model, to investigate these connections.

## Past Projects (see [GitHub](https://github.com/TrentBrick))

* [RewardConditionedUDRL](https://github.com/TrentBrick/RewardConditionedUDRL) - Fall 2020 - Open source codebase combining implementations of [Reward Conditioned Policies](https://arxiv.org/pdf/1912.13465.pdf) and [Training Agents using Upside-Down Reinforcement Learning](https://arxiv.org/abs/1912.02877). The former had no public implementation and the latter had a few implemented as Jupyter Notebooks but that had a number of issues I flagged eg. [here](https://github.com/jscriptcoder/Upside-Down-Reinforcement-Learning/issues/1) and [here](https://github.com/BY571/Upside-Down-Reinforcement-Learning/issues/4#event-3624848392). I hope this open source codebase will serve to both fully replicate the aforementioned papers and be used as a starting point for further research in the exciting domain of supervised RL.

* [SARS-CoV-2 mutation effects and 3D structure prediction from sequence covariation.](https://marks.hms.harvard.edu/sars-cov-2) - Summer 2020 - Collaborated with the Marks lab to help produce their SARS-CoV-2 mutation effect and 3D structure predictions using EVCouplings. <br>
<br>
Citation: <br>
Nathan Rollins\*, Kelly Brock\*, Joshua Rollins\*, Judy Shen, Amy Tam, Ada Shaw, ***Trenton Bricken***, Augustin Luna, Nicholas Gauthier, Thomas Hopf, Chris Sander, Debora Marks. (2020)
<br>
Website: <https://marks.hms.harvard.edu/sars-cov-2>

* RL Learning Byzantine Fault Tolerant (BFT) Consensus Protocols - Senior Year - Supervised by Dr. Kartik Nayak, final class project turned research project. Investigated the ability for deep reinforcement learning agents to discover and prove BFT consensus protocols.

* Protein Generation and Optimization - Supervised by Dr. Debora Marks's Lab as my Senior Thesis - This research is motivated by the promise of recent developments in our ability to predict protein functionality and the problem of finding novel sequences that maximize this prediction. We tried developing a new solution using invertible neural networks and variational inference to approximate the intractable distribution of any protein function predictor with reason to believe it would outperform Markov Chain Monte Carlo methods. My senior thesis write up of the work and where it seemed to succeed and fail is forthcoming.

* [PyTorch Discrete Normalizing Flows](https://github.com/TrentBrick/PyTorchDiscreteFlows) - Winter Break 2019 - Learning about Discrete Normalizing Flows from "Discrete Flows: Invertible Generative Models of Discrete Data", by Dustin Tran et al. <https://arxiv.org/pdf/1905.10347.pdf>, I tried implementing them using the coded provided in [edward2](https://github.com/google/edward2/tree/master/edward2/tensorflow/layers#4-reversible-layers) but found that [none of it worked](https://github.com/google/edward2/issues/148). I ended up porting all of the code into PyTorch which required making a number of modifications and getting it working on a toy example. This repo as of April 2021 has 63 Github stars and I have had two collaborators help me replicate the results.

* [Tail Free Sampling](https://trentbrick.github.io/Tail-Free-Sampling/) - Independent project, advice from friends and mentors - Developed a new method to sample sequences from autoregressive neural networks for open-ended sequence generation.

* [Primary and Tertiary Protein AutoEncoder](https://github.com/TrentBrick/PAE) - Final Class Project - Investigated if a deep AutoEncoder could learn the relationship between protein sequence and tertiary structure in order to then do either sequence or structure optimization in the latent space. It didn't work very well but I learned a lot!

* [Facebook Chatbot for Spaced Repetition Learning](https://github.com/TrentBrick/MMRY) - HackDuke 2016 - Spaced Repetition is [wonderful and highly neglected](https://www.gwern.net/Spaced-repetition). Can we make it more popular and easy to do routinely using a Facebook Chatbot to both harass and motivate us? Got everything working! But there were always more bugs and this didn't solve the fundamental problem of spaced repetition learning still taking a huge amount of motivation. You could argue that presenting the cards over Messenger just created more distractions.  

### Other Locations on the Interwebs

I am pretty active on [Twitter](https://twitter.com/TrentonBricken) and sometimes upload my film photography to [Instagram](https://www.instagram.com/blitz_analog) and to my [portfolio](https://blitz-analog.github.io/).

### Get in Touch

If any of the things I have mentioned are interesting to you please reach out! I love to meet new people.

[brickentrenton@gmail.com](mailto:brickentrenton@gmail.com)

### Anonymous Feedback

Have any feedback for me? Please consider filling out this anonymous feedback form so that I can learn and grow :)

<https://forms.gle/VcY3vQgkdf6c69dr7>
