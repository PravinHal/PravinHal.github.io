---
layout: post
title: Book Review - After Phrenology
date: 2021-3-14 22:49:0 +0000
comments: True
categories: Neuroscience
---

After Phrenology by Michael L. Anderson makes three main arguments (I have ordered by compellingness):

1. Brain regions are functionally *differentiated* but not functionally *specialized*. Each brain region is differentiated, just as a hammer is different to a knife; however, each region still contributes to many different functions just as a knife can be used to not only cut vegetables but also spread butter and tighten screws. Anderson reviews research showing that any specific brain region is involved in a large number of different cognitive operations. This is counter to our focus on modular localized cognition that [fMRI](https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging) emphasizes and has carried on since the practice of [phrenology](https://en.wikipedia.org/wiki/Phrenology).

2. All cognition and our study of it should be action oriented and “embodied”. All cognition interfaces with our environment and ensures our survival through the production of action. There is no point in having senses if no action can be taken to respond to them. Anderson uses this interface with our environment and bodies to argue against popular symbol based theories of cognition. In addition, accounting for environmental and biological constraints reduces the degrees of freedom for symbolic representations and learning, making tasks less dependent on complex symbolic manipulation.

3. The brain's abilities are made possible by its dynamic functional connections. These connections are both between other brain regions and the external environment. Our cognition is occurring at four different levels each with unique properties: genetic, synaptic, chemical, and environmental. The chemical level refers to extrasynaptic chemical gradients for signalling via [volume transmission](https://pubmed.ncbi.nlm.nih.gov/23674109/#:~:text=Volume%20transmission%20(VT)%20is%20a,diffusion%20and%20convection%20(flow)) that I was unaware of (and I suspect may also be true for other readers).

## Take-Aways

I believe After Phrenology effectively hammered home a deeper appreciation for just how complex the task of unravelling the secrets of the brain will be. A favourite phrase from a friend “Evolution did not optimize for interpretability” comes to mind as evolution goes about its greedy optimization, creating a mish-mash of interdependencies between and across the brain components it has at hand. Keeping more in mind just how interconnected the brain is both within itself and with our environment will be useful for future research on cognition. This includes in general having more awareness of our collective bias to assign single modular functions to each component of any system.

After Phrenology has made me more skeptical of fMRI than I already was. It has also made me more interested in volume transmission and the dynamic routing between brain modules. This dynamic routing reminds me of [this](https://arxiv.org/abs/2101.03961) recent deep learning paper that successfully leveraged dynamic switching between a number of neuronal submodules. I found the critique of symbol based processing in the brain to provide a refreshing and new perspective.

However, I feel like there are fundamental definitional problems with this discussion that are emblematic of my main problem with the book being too high level and often avoiding any specific details about mechanisms or implementation. In trying to resolve these definitional issues, I am ironically more bullish about Vector *Symbolic* Architectures (VSAs) than I was before reading this critique. It seems to me that VSAs can address the symbol based issues outlined here, while also fitting very nicely with a biologically plausible implementation of Turing complete symbolic systems as noted in [this paper](http://colala.berkeley.edu/papers/piantadosi2020computational.pdf) and shown to work powerfully in [this work](https://www.sciencedirect.com/science/article/pii/S0004370220301855). Update: I have since read *[How to Build a Brain](https://smile.amazon.com/How-Build-Brain-Architecture-Architectures/dp/0190262125?sa-no-redirect=1)* which further supports my argument here.

The focus on action oriented learning and embodied cognition was appreciated and fit well with my general beliefs in [evolutionary biology](https://trentbrick.github.io/GreatestHost/), the [Free Energy Principle](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006421) and its [Active Inference](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007805). Action orientation does have interesting implications for deep learning that are supported by [this]((https://arxiv.org/abs/2101.03961)) recent DeepMind paper that emphasizes how all symbols must have utility for an end task and be created by and have subjective value to the agent itself, not to us as humans. This paper also acknowledges one great way to learn symbols might be through social interaction as pursued in [this paper](https://arxiv.org/pdf/2012.05672.pdf), which fits with After Phrenology’s arguments that all language is for the purpose of socialization. I believe it is also noteworthy that DeepMind’s [MuZero](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) does not decode any of its symbolic reconstructions back into pixels. This is in stark contrast to many model based deep reinforcement learning approaches like [this](https://worldmodels.github.io/) and [this](https://danijar.com/project/dreamer/) where the symbolic representation the agent learns is forced to reconstruct its input scene[^problemFEP].   
Beyond not forcing the learners to have symbolic representations that are interpretable to us as humans, the action oriented perspective also rejects modelling parts of the world that are not relevant to action and by proxy survival. [This paper](https://pathak22.github.io/noreward-rl/) cleverly frames this problem as forcing the agent to only learn about things that it can directly control or be affected by. For example, a land based creature does not very often not need to use its limited computational resources to model with pixel level precision a cloud being blown across the sky. One natural and successful way of creating this action oriented approach is by simply trying to predict the next thing which has led to a number of very impressive successes, [this](https://www.nature.com/articles/s41592-019-0598-1) and [this](https://openai.com/blog/dall-e/) paper come most readily to mind.


## Symbolic Processing

Turning to Anderson’s problems with symbolic processing in the brain, his argument rests on symbols being unnecessary to construct and already existing in our environment. Anderson argues that a source of many symbolic beliefs about the brain come from the perceived difficulty of inferring sensory stimuli: converting 2D, noisy images of our surroundings into a vibrant 3D reconstruction of the world around us is so difficult that we must use inference to reconstruct an internal representation of the world around us. Moreover, not keeping the embodied and action oriented nature of cognition in mind, it is easy to think of the brain as an independent representation machine that takes in sensory input and performs many computations on them like our computers.

Anderson argues that when we account for the embodied nature of cognition, sensory data is much easier to interpret, obviating the need for an internal reconstruction. In addition, the environment itself can be used for many forms of memory and symbolic processing. Because we experience the world in continuous time, we receive a huge amount of redundant *temporal* data that makes sensory inference easier. For example a single image of a mug on my desk appearing in my retina may make it difficult to determine it is a cup, however, many images of this cup over the span of seconds from different angles and [saccades](https://en.wikipedia.org/wiki/Saccade) and in the context of my office makes it much easier. Moreover, when we are reacting to parts of the world and deciding how to respond, our physical body and environment constrains many degrees of freedom making decisions and processing more straightforward. Similarly, while many languages can perform infinite recursion, in practice we struggle to use more than three nested statements, something that is quite tractable for language models to account for.  

With regards to symbols existing in our environment rather than in our brains, Anderson reviews evidence about performing mathematics where much of our symbolic manipulation uses visual and tactile heuristics, a far cry from siloed symbolic manipulation. To ground all of this theory, Anderson highlights how humans catch a ball using the “optical acceleration cancellation” heuristic rather than complex physics. This heuristic works by moving to cancel out the perceived vertical acceleration of the ball. We perform this operation in a closed loop between our perception of the ball and physical coordinates in space without needing to rely upon any symbols. As another example, Anderson talks about the [Centrifugal governor](https://en.wikipedia.org/wiki/Centrifugal_governor) which is a clever feedback device to make the speed of an engine constant. The device operates in a dynamic, closed loop that can be described by a differential equation. However, we could also modularize each component of the device and use symbols to compute how each part of it relates to the others. This is the discrepancy between the symbol-free and symbol based views of cognition that I believe also apply to the difference between model free and model based reinforcement learning. To date, the model free programs used by many RL agents have been largely the most successful.

Anderson goes even further to argue that the holy-grail of symbolic processing -- language -- is in fact also very much a symbol free dynamical system that is oriented towards social, conversational settings. Anderson argues that language may in fact be more easy to learn that we give assume because it evolves on top of human capacities where only language that is easy to learn and use will propagate. Moreover, in social settings there are many conversational and behavioural norms that are followed to enable successful communication (eg. tone and posture mirroring) that constrain the degrees of freedom for what to say.  My own poor understanding of English grammar yet ability to communicate and still write (with questionable success for certain!) comes to mind as another way in which I have learnt the conventions of English without actually having a robust symbolic and self contained logical understanding of how English or language as a whole functions.

While these examples are interesting and it makes sense that we form many heuristics rather than understanding the inner workings of the world, I do not believe Anderson’s critiques conflict with all forms of symbolic processing. We may not have unique, isolated symbols for every thought and calculation performed under one logical framework. However, even in Anderson’s differential equation used to catch a ball there are variables corresponding to the ball and our body’s position in space that will be represented by some neural firing pattern and can be considered symbols. In fact, Anderson indirectly acknowledges this fact by talking about different variables in these equations being represented by vectors made up of [neural population codes](https://en.wikipedia.org/wiki/Neural_coding#:~:text=Population%20coding%20is%20a%20method,some%20value%20about%20the%20inputs.). Moreover, he cites work by [Smolesky](https://web.stanford.edu/~jlmcc/papers/PDP/Volume%202/Chap22_PDP86.pdf), one of the founders of Vector *Symbolic* Architectures (VSAs) for how these vectors can be combined into superpositions to represent probabilistic outputs[^VSAs].  Are these variables that can be combined with mathematical operations not symbols? In addition, while the environment may be used as an external symbol system, Anderson says nothing about memories and how they are stored and recalled.

Beyond Anderson’s critiques being compatible with VSAs, I remain skeptical of the dynamic equation/model-free argument for all learning and responses. Taking compelling arguments from [this](https://arxiv.org/pdf/2008.08814.pdf)[^socratic], dynamical systems are highly limited in their computational capacities while Turing machines are not. Moreover, Turing machines are easy to create (they include cellular automata such as [Conways’s game of life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life) and [Wolfram’s Rule 110](https://en.wikipedia.org/wiki/Rule_110)) and could hypothetically be [implemented](https://arxiv.org/pdf/2008.08814.pdf) through only four simple enzymes performing [combinatory logic](https://en.wikipedia.org/wiki/Combinatory_logic) on strands of RNA. As a result, if there were compelling reasons for evolution to create a universal Turing machine, it would be very tractable for it to do so.  


## Other interesting facts

On the dynamic utilization of neural resources: It was known that the visual cortex of the brain is utilized in braille patients and that this utilization is functional as targeted [TMS](https://en.wikipedia.org/wiki/Transcranial_magnetic_stimulation) was able to interfere with performance. An interesting [experiment](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2516172/) was done where sighted people were kept in total darkness for five days straight. During this time they were taught braille and within the first few days their visual cortex also appeared to be utilized for braille and was functionally important. However, when they returned to light the visual cortex was no longer used for braille processing. However, their performance and what they learned remained robust and unchanged.

We know that there are connections between tactile and visual regions of the brain so these connections were not de novo but much stronger than usual. This dynamic utilization of extra cognitive resources is impressive in the speed at which it occurred. It also begs the question of whether or not it allowed the participants to learn braille faster than they otherwise would have? Or to retain it better? What are the functional benefits from being able to use more neural resources during this period?

One proposed way in which this dynamic computation can occur is via volume transmission as a form of extrasynaptic signalling. One interesting [study](https://onlinelibrary.wiley.com/doi/10.1002/hipo.1101) found that a reduction in hippocampal anisotropy (the variance of diffusion that occurs in different directions) was related to a reduction in learning ability for rats tested on the Morris water maze. The degree of anisotropy [has also been shown](https://pubmed.ncbi.nlm.nih.gov/15561404/) to be related to aging and neurological disorders.

Pyramidal neurons have synapses with only 1% of neurons in the range of their dendrites.

Only 5-20% of input in the primary visual cortex comes from sensory input, the rest are recurrent connections from elsewhere. The brain is highly recurrent.

I am more skeptical about [whole brain emulation](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf) than I used to be. Even if you could record every single synapse in the human brain, you would also need to account for extrasynaptic signalling. You would also need to model neuron-glia interactions. And to the extent that embodied cognition and symbols are stored in the environment, this would also need to be taken into account.

Narwhal tusks which I had previously heard have unknown utility, are in fact a sense organ tuned to salinity differentials that specify the freezing of the water's surface overhead.

### Footnotes
* footnotes will be placed here. This line is necessary
{:footnotes}

[^problemFEP]: Interestingly, this is also a problem with the Free Energy Principle using Variational Inference in the way it [currently does](https://arxiv.org/abs/1709.02341). While this does enable a probabilistic model of observations to be built, it seems like this representation should not be learnt at the pixel level and should instead learn a probabilistic model in an autoregressive fashion using the chain rule of probability.

[^VSAs]: For a very interesting introduction to VSAs and their symbolic potential that I am very bullish on see [this excellent paper](http://rctn.org/vs265/kanerva09-hyperdimensional.pdf).

[^socratic]: See [this](http://www.lifeiscomputation.com/breaking-free-from-neural-networks-and-dynamical-systems/) for a Socratic dialogue of the same arguments.
